{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle report for tweet archive of Twitter user @dog_rates data\n",
    "Author: Jem Chang  \n",
    "Date: 11Mar2019  \n",
    "Purpose: document data wrangling process including gathering, assessing, and cleaning data  \n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#env\">Environment and tools</a></li>\n",
    "<li><a href=\"#gather\">Data Gathering</a></li>\n",
    "<li><a href=\"#assess\">Data Assessing and Cleaning</a></li>\n",
    "<li><a href=\"#summary\">Summary</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='env'></a>\n",
    "#### Environment and Tools\n",
    "The data wrangling process is peformed in the Jupyter Notebook with Python 3.7. The libraries used in this project are pandas, requests, tweepy, json, re, default_timer, matplotlib.pyplot, seaborn, and scipy.stats. %matplotlib inline is added for direct outputs in the notebook. pd.options.display.max_colwidth = 600 is set for avoiding text collapes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "#### Data Gathering  \n",
    "The datasets for this projects are from the tweet archive of Twitter user @dog_rates (WeRateDogs).    \n",
    "\n",
    "1. Enhanced Twitter Archive: contains tweet data for all 5000+. Only 2356 records have ratings.   \n",
    "File name: twitter-archive-enhanced  \n",
    "Format: csv  \n",
    "Source: directly download from Udacity website.  \n",
    "\n",
    "2. Image Predictions File: the output from neural network   \n",
    "File name: image-predictions  \n",
    "Format: tsv  \n",
    "Source: get the data from url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'    \n",
    "\n",
    "| Variable Name  | Definition                                                                                  |\n",
    "|----------------|---------------------------------------------------------------------------------------------|\n",
    "| tweet_id       | the last part of the tweet URL after \"status/\"                                              |\n",
    "| p1             | the algorithm's #1 prediction for the image in the tweet                                    |\n",
    "| p1_conf        | how confident the algorithm is in its #1 prediction                                         |\n",
    "| p1_dog         | whether or not the #1 prediction is a breed of dog                                          |\n",
    "| p2             | the algorithm's #2 prediction for the image in the tweet                                    |\n",
    "| p2_conf        | how confident the algorithm is in its #2 prediction                                         |\n",
    "| p2_dog         | whether or not the #2 prediction is a breed of dog                                          |\n",
    "| p3             | the algorithm's #3 prediction for the image in the tweet                                    |\n",
    "| p3_conf        | how confident the algorithm is in its #3 prediction                                         |\n",
    "| p3_dog         | whether or not the #3 prediction is a breed of dog                                          |\n",
    "\n",
    "3. Additional Data via the Twitter API  \n",
    "File name: tweet_json  \n",
    "Format: txt  \n",
    "Source: connect Twitter API to download json format text file and use pandas to read into the notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "#### Data Assessing and Cleaning\n",
    "##### Quality\n",
    "`Enhanced Twitter Archive` table:  \n",
    "1. The column, text, includes url: the rating, name, and stage might be not all correct, so removing urls is easier for the future cleaning.  \n",
    "2. The ratings are not all correct: some of ratings are only extracted the decimals instead of the full numbers. I clean numerators and denomiators separatly and set up the cutoffs to filter bigger numbers. And I visually review them and find the logic that causes the incorrect extraction.  \n",
    "3. The denominator of the ratings should not be 0: originally, there are some 0 denominators in the dataset but after correcting rating, the 0's all gone.  \n",
    "4. Timestamp is datetime instead of string and the timestamp later than August 1st, 2017 should be removed because there is no image prediction output after the date. \n",
    "5. This table should only keep original ratings that have images for the future analysis.\n",
    "6. Dog stages (doggo, floofer, pupper, puppo) might be not all correct. After checking the stages, they are all extracted correctly from the text.\n",
    "7. The names are not all correct: I only fix one issue by using IG account as a name if name is missing or is extracted incorrectly. I don't think it will influence the future analysis so I leave others as is. \n",
    "\n",
    "`Additional Data via the Twitter API` table:  \n",
    "8. id should be renamed as tweet_id to be consistent with other ids from different tables for the future joining/merging with other datasets.\n",
    "\n",
    "##### Tidiness  \n",
    "Based on tidy data rules: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html  \n",
    "\n",
    "`Enhanced Twitter Archive` table: column headers are values, not variable names \n",
    "1. All the dog stages (doggo, floofer, pupper, puppo) should be transposed into one column, dog_stage. If there are more than one stages, the stages will be concatenated and separated by one space.  \n",
    "\n",
    "`Image Predictions File` table: column headers are values, not variable names  \n",
    "2. All p1, p2, p3, p1_cof, p2_cof, p3_cof, p1_dog, p2_dog, and p3_dog should be transposed into p, p_conf, p_dog. \n",
    "Note: after tidying image prediction table, I do not merge back to tweeter archive file because it will cause that the observations of tweeter archive files increase three times. I only merge them when I perform the analysis instead of the storing data stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "#### Summary  \n",
    "The most challenging part of the wrangling process is extracting useful information from each tweet. Regular expression is heavily used in this process. After data wrangling, the datasets are ready for analysis and modeling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
